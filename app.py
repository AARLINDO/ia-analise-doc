import streamlit as st
import google.generativeai as genai

# ==============================================================================
# CONFIGURA√á√ÉO E CHAVE
# ==============================================================================
# COLE SUA CHAVE AQUI DENTRO DAS ASPAS
CHAVE_MESTRA = "AIzaSyDKSC9mAkeodr96m6SgcCvn70uZHseiM4A" 

st.set_page_config(page_title="Carm√©lio AI", page_icon="‚öñÔ∏è", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #0E1117; }
    .stButton>button { background: linear-gradient(45deg, #4285F4, #9B72CB); color: white; border: none; font-weight: bold; }
    h1, h2, h3 { color: #E0E0E0; }
</style>
""", unsafe_allow_html=True)

# ==============================================================================
# C√âREBRO DA IA (COM SISTEMA ANTI-ERRO)
# ==============================================================================
def get_gemini_response(mode, user_input, file_data=None, mime_type=None):
    if "COLE_SUA" in CHAVE_MESTRA:
        return "‚ö†Ô∏è Erro: Voc√™ esqueceu de colocar a chave no c√≥digo (linha 8)!"
        
    genai.configure(api_key=CHAVE_MESTRA)
    
    # Defini√ß√£o das Personas
    prompts = {
        "padrao": "Voc√™ √© um assistente jur√≠dico √∫til.",
        "oab": "ATUE COMO: Examinador OAB. Exija fundamenta√ß√£o legal (CLT/S√∫mulas).",
        "pcsc": "ATUE COMO: Mentor PCSC. Destaque pegadinhas da banca FGV/Cebraspe."
    }
    instruction = prompts.get(mode, "padrao")
    
    # Tenta usar o modelo 1.5 (Mais Inteligente)
    try:
        model = genai.GenerativeModel("gemini-1.5-flash", system_instruction=instruction)
        content = [user_input]
        if file_data: content.append({"mime_type": mime_type, "data": file_data})
        return model.generate_content(content).text
    except Exception as e_flash:
        # Se der erro 404, tenta o modelo PRO (Mais Compat√≠vel)
        try:
            model = genai.GenerativeModel("gemini-pro") # Vers√£o compat√≠vel antiga
            # O modelo antigo n√£o aceita system_instruction direto, ent√£o injetamos no texto
            full_prompt = f"INSTRU√á√ÉO DO SISTEMA: {instruction}\n\nUSU√ÅRIO: {user_input}"
            content = [full_prompt]
            if file_data: 
                return "‚ö†Ô∏è O modelo antigo (Gemini Pro) n√£o aceita arquivos. Tente apenas texto ou reinicie o app."
            return model.generate_content(content).text
        except Exception as e_pro:
            return f"Erro Fatal: {e_flash} | Tentativa Backup: {e_pro}"

# ==============================================================================
# INTERFACE
# ==============================================================================
st.title("‚ú® Carm√©lio AI: Gemini Power")

with st.sidebar:
    st.success("‚úÖ Chave Conectada")
    modo_visual = st.radio("Modo:", ["ü§ñ Geral", "‚öñÔ∏è Mentor OAB", "üöì Mentor PCSC"])
    modo_map = {"ü§ñ Geral": "padrao", "‚öñÔ∏è Mentor OAB": "oab", "üöì Mentor PCSC": "pcsc"}
    
    if st.button("üóëÔ∏è Limpar Conversa"):
        st.session_state['chat_history'] = []
        st.rerun()

if 'chat_history' not in st.session_state: st.session_state['chat_history'] = []

for msg in st.session_state['chat_history']:
    with st.chat_message(msg['role'], avatar="üë§" if msg['role'] == "user" else "ü§ñ"):
        st.markdown(msg['content'])

col1, col2 = st.columns([0.85, 0.15])
with col1: prompt = st.chat_input("Digite sua d√∫vida...")
with col2: uploaded_file = st.file_uploader("üìé", type=["png", "jpg", "pdf"], label_visibility="collapsed")

if prompt:
    st.session_state['chat_history'].append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="üë§"): st.markdown(prompt)
    
    file_bytes = None; mime = None
    if uploaded_file:
        file_bytes = uploaded_file.getvalue(); mime = uploaded_file.type
        st.info(f"Analisando arquivo: {uploaded_file.name}...")

    with st.chat_message("assistant", avatar="ü§ñ"):
        with st.spinner("Pensando..."):
            resp = get_gemini_response(modo_map[modo_visual], prompt, file_bytes, mime)
            st.markdown(resp)
            st.session_state['chat_history'].append({"role": "assistant", "content": resp})
